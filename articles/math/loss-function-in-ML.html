<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>损失函数</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="/static/custom.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">损失函数</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#经验风险与结构风险"><span class="toc-section-number">1</span> 经验风险与结构风险</a><ul>
<li><a href="#一般误差与经验误差"><span class="toc-section-number">1.1</span> 一般误差与经验误差</a><ul>
<li><a href="#马尔可夫不等式"><span class="toc-section-number">1.1.1</span> 马尔可夫不等式</a></li>
<li><a href="#切比雪夫不等式"><span class="toc-section-number">1.1.2</span> 切比雪夫不等式</a></li>
</ul></li>
<li><a href="#ermempirical-risk-minimization"><span class="toc-section-number">1.2</span> ERM(Empirical Risk Minimization)</a></li>
<li><a href="#srmstructural-risk-minimization"><span class="toc-section-number">1.3</span> SRM(Structural Risk Minimization)</a></li>
<li><a href="#参考"><span class="toc-section-number">1.4</span> 参考</a></li>
</ul></li>
<li><a href="#损失函数与风险函数"><span class="toc-section-number">2</span> 损失函数与风险函数</a></li>
<li><a href="#参考-1"><span class="toc-section-number">3</span> 参考</a></li>
</ul>
</nav>
<h1 id="经验风险与结构风险"><span class="header-section-number">1</span> 经验风险与结构风险</h1>
<h2 id="一般误差与经验误差"><span class="header-section-number">1.1</span> 一般误差与经验误差</h2>
<ul>
<li>一般误差：模型在任意样本下得到的误差，也就是模型的真实误差；</li>
<li>经验误差：训练样本的误差，由于是基于训练样本的，也就是基于已知经验的，称为经验误差。</li>
</ul>
<p>机器学习的目标是最小化一般误差，但是由于是NP难题，因此一般转而最小化经验误差（ERM），但是这种方式并非是最好的（典型的比如过拟合），也就是说经验误差来代替一般误差不是最优的。</p>
<h3 id="马尔可夫不等式"><span class="header-section-number">1.1.1</span> 马尔可夫不等式</h3>
<p>马尔可夫不等式给出了随机变量的函数大于等于某正数的概率的上界，其数学表述为：<span class="math inline">\(P(X{\ge}a){\le}\frac{E(X)}{a}\)</span></p>
<h3 id="切比雪夫不等式"><span class="header-section-number">1.1.2</span> 切比雪夫不等式</h3>
<p>切比雪夫不等式是马尔可夫不等式的特殊情况，描述了这样一个事实，事件大多会集中在均值附近。其数学表述为： <span class="math inline">\(P(|X-\mu|{\ge}k\sigma)\le\frac{1}{k^2}\)</span>其中<span class="math inline">\(\mu\)</span>是期望，<span class="math inline">\(\sigma\)</span>是标准差。</p>
<p>文字解释为：</p>
<blockquote>
<p>任意一个数据集中，位于其平均数k个标准差范围内的比例（或部分）总是至少为<span class="math inline">\(1-1/k_2\)</span>，其中k为大于1的任意正数。</p>
<p>对于k=2，k=3和k=5有如下结果：</p>
<ul>
<li>所有数据中，至少有3/4（或75%）的数据位于平均数2个标准差范围内。</li>
<li>所有数据中，至少有8/9（或88.9%）的数据位于平均数3个标准差范围内。</li>
<li>所有数据中，至少有24/25（或96%)的数据位于平均数5个标准差范围内</li>
</ul>
<p>作者：马horse 链接：https://www.zhihu.com/question/27821324/answer/92675490 来源：知乎 著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>
</blockquote>
<h2 id="ermempirical-risk-minimization"><span class="header-section-number">1.2</span> ERM(Empirical Risk Minimization)</h2>
<ul>
<li>经验误差存在上限，是其一般误差比较好的估计；</li>
<li>选择最小经验误差的拟合函数，其与一般误差函数的差异不超过2r。</li>
</ul>
<h2 id="srmstructural-risk-minimization"><span class="header-section-number">1.3</span> SRM(Structural Risk Minimization)</h2>
<p>综合ERM与置信范围，寻求一个折中的最佳点。</p>
<h2 id="参考"><span class="header-section-number">1.4</span> 参考</h2>
<ul>
<li><a href="https://blog.csdn.net/u013709270/article/details/53997686">经验风险与结构风险</a></li>
</ul>
<h1 id="损失函数与风险函数"><span class="header-section-number">2</span> 损失函数与风险函数</h1>
<p>损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏。</p>
<p>损失函数是<span class="math inline">\(f(X)\)</span>和Y的非负实值函数，记做L(Y,f(X))。损失函数是经验风险函数的核心部分，也是结构风险函数的重要组成部分，模型的风险结构包括了风险项和正则项： <span class="math inline">\(\theta^*=argmin_{\theta}\frac{1}{N}\sum_{i=1}^{N}L(y_i,f(x_i;\theta))+\lambda \Phi(\theta)\)</span>其中<span class="math inline">\(\Phi\)</span>是正则化项或者惩罚项，可以是L1，也可以是L2或者其他正则函数。</p>
<p>常用的损失函数：</p>
<ol type="1">
<li>0-1损失函数：<span class="math inline">\(L(Y,f(X))= \begin{cases}1,&amp;\text{$Y{\ne}f(X)$}\\0,&amp;\text{$Y=f(X)$}\end{cases}\)</span></li>
<li>平方损失函数（quadratic loss function）：<span class="math inline">\(L(Y,f(X))=(Y-f(X))^2\)</span></li>
<li>绝对损失函数：<span class="math inline">\(L(Y,f(X))=|Y-f(X)|\)</span></li>
<li>对数损失函数：<span class="math inline">\(L(Y,P(Y|X))=-logP(Y|X)\)</span></li>
</ol>
<h1 id="参考-1"><span class="header-section-number">3</span> 参考</h1>
<ul>
<li>李航，《统计学习方法》</li>
<li><a href="https://blog.csdn.net/u010976453/article/details/78488279">机器学习中的损失函数（着重比较：hinge loss vs softmax loss）</a></li>
</ul>
</body>
</html>
