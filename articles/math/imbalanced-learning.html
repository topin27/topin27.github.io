<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Imbalanced Learning</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="/static/custom.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Imbalanced Learning</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#learning-from-imbalanced-classes"><span class="toc-section-number">1</span> Learning from Imbalanced Classes</a><ul>
<li><a href="#handling-imbalanced-data"><span class="toc-section-number">1.1</span> Handling imbalanced data</a></li>
<li><a href="#digression-evaluation-dos-and-donts"><span class="toc-section-number">1.2</span> Digression: evaluation dos and don’ts</a></li>
<li><a href="#oversampling-and-undersampling"><span class="toc-section-number">1.3</span> Oversampling and undersampling</a></li>
<li><a href="#bayesian-argument-of-wallace-et-al."><span class="toc-section-number">1.4</span> Bayesian argument of Wallace et al.</a></li>
<li><a href="#neighbor-based-approaches"><span class="toc-section-number">1.5</span> Neighbor-based approaches</a></li>
<li><a href="#synthesizing-new-examples-smote-and-descendants"><span class="toc-section-number">1.6</span> Synthesizing new examples: SMOTE and descendants</a></li>
<li><a href="#adjusting-class-weights"><span class="toc-section-number">1.7</span> Adjusting class weights</a></li>
<li><a href="#and-beyond"><span class="toc-section-number">1.8</span> And beyond</a><ul>
<li><a href="#new-algorithms"><span class="toc-section-number">1.8.1</span> New algorithms</a></li>
<li><a href="#buying-or-creating-more-data"><span class="toc-section-number">1.8.2</span> Buying or creating more data</a></li>
</ul></li>
</ul></li>
</ul>
</nav>
<h1 id="learning-from-imbalanced-classes"><span class="header-section-number">1</span> Learning from Imbalanced Classes</h1>
<p>Post original here: <a href="https://www.svds.com/learning-imbalanced-classes/" class="uri">https://www.svds.com/learning-imbalanced-classes/</a></p>
<h2 id="handling-imbalanced-data"><span class="header-section-number">1.1</span> Handling imbalanced data</h2>
<p>That said, here is a rough outline of useful approaches. These are listed approximately in order of effort:</p>
<ul>
<li>Do nothing. Sometimes you get lucky and nothing needs to be done. You can train on the so-called natural (or stratified) distribution and sometimes it works without need for modification.</li>
<li>Balance the training set in some way:
<ul>
<li>Oversample the minority class.</li>
<li>Undersample the majority class.</li>
<li>Synthesize new minority classes.</li>
</ul></li>
<li>Throw away minority examples and switch to an anomaly detection framework.</li>
<li>At the algorithm level, or after it:
<ul>
<li>Adjust the class weight (misclassification costs).</li>
<li>Adjust the decision threshold.</li>
<li>Modify an existing algorithm to be more sensitive to rare classes.</li>
</ul></li>
<li>Construct an entirely new algorithm to perform well on imbalanced data.</li>
</ul>
<h2 id="digression-evaluation-dos-and-donts"><span class="header-section-number">1.2</span> Digression: evaluation dos and don’ts</h2>
<ol type="1">
<li>Don’t use accuracy (or error rate) to evaluate your classifier.</li>
<li>Don’t get hard classifications (labels) from your classifier (via <code>score</code> or <code>predict</code>). Instead, get probability estimates via <code>proba</code> or <code>predict_proba</code>.</li>
<li>When you get probability estimates, don’t blindly use a 0.50 decision threshold to separate classes. Look at performance curves and decide for yourself what threshold to use (see next section for more on this). Many errors were made in early papers because researchers naively used 0.5 as a cut-off.</li>
<li>No matter what you do for training, always test on the natural (stratified) distribution your classifier is going to operate upon. See <code>sklearn.cross_validation.StratifiedKFold</code>.</li>
<li>You can get by without probability estimates, but if you need them, use calibration (see <code>sklearn.calibration.CalibratedClassifierCV</code>).</li>
</ol>
<p>One of these is preferable to accuracy:</p>
<ol type="1">
<li>The Area Under the ROC curve (AUC) is a good general statistic. It is equal to the probability that a random positive example will be ranked above a random negative example.</li>
<li>The F1 Score is the harmonic mean of precision and recall. It is commonly used in text processing when an aggregate measure is sought.</li>
<li>Cohen’s Kappa is an evaluation statistic that takes into account how much agreement would be expected by chance.</li>
</ol>
<h2 id="oversampling-and-undersampling"><span class="header-section-number">1.3</span> Oversampling and undersampling</h2>
<h2 id="bayesian-argument-of-wallace-et-al."><span class="header-section-number">1.4</span> Bayesian argument of Wallace et al.</h2>
<p>Use bagging to combine classifiers.</p>
<h2 id="neighbor-based-approaches"><span class="header-section-number">1.5</span> Neighbor-based approaches</h2>
<p>Over- and undersampling selects examples randomly to adjust their proportions. Other approaches examine the instance space carefully and decide what to do based on their neighborhoods.</p>
<p>For example, Tomek links are pairs of instances of opposite classes who are their own nearest neighbors. In other words, they are pairs of opposing instances that are very close together.</p>
<p>Tomek’s algorithm looks for such pairs and removes the majority instance of the pair. The idea is to clarify the border between the minority and majority classes, making the minority region(s) more distinct.</p>
<h2 id="synthesizing-new-examples-smote-and-descendants"><span class="header-section-number">1.6</span> Synthesizing new examples: SMOTE and descendants</h2>
<h2 id="adjusting-class-weights"><span class="header-section-number">1.7</span> Adjusting class weights</h2>
<p>Many machine learning toolkits have ways to adjust the “importance” of classes. Scikit-learn, for example, has many classifiers that take an optional <code>class_weight</code> parameter that can be set higher than one.</p>
<p>It should be noted that adjusting class importance usually only has an effect on the cost of class errors (False Negatives, if the minority class is positive). It will adjust a separating surface to decrease these accordingly. Of course, if the classifier makes no errors on the training set errors then no adjustment may occur, so altering class weights may have no effect.</p>
<h2 id="and-beyond"><span class="header-section-number">1.8</span> And beyond</h2>
<h3 id="new-algorithms"><span class="header-section-number">1.8.1</span> New algorithms</h3>
<p>In 2014 Goh and Rudin published a paper Box Drawings for Learning with Imbalanced Data which introduced two algorithms for learning from data with skewed examples. These algorithms attempt to construct “boxes” (actually axis-parallel hyper-rectangles) around clusters of minority class examples.</p>
<p>Their goal is to develop a concise, intelligible representation of the minority class. Their equations penalize the number of boxes and the penalties serve as a form of regularization.</p>
<p>They introduce two algorithms, one of which (Exact Boxes) uses mixed-integer programming to provide an exact but fairly expensive solution; the other (Fast Boxes) uses a faster clustering method to generate the initial boxes, which are subsequently refined. Experimental results show that both algorithms perform very well among a large set of test datasets.</p>
<h3 id="buying-or-creating-more-data"><span class="header-section-number">1.8.2</span> Buying or creating more data</h3>
</body>
</html>
