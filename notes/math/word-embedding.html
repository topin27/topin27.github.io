<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Word Embedding</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="/static/custom.css" />
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Word Embedding</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#glove-global-vector"><span class="toc-section-number">1</span> GloVe (GLObal VEctor)</a><ul>
<li><a href="#参考"><span class="toc-section-number">1.1</span> 参考</a></li>
</ul></li>
<li><a href="#word2vec"><span class="toc-section-number">2</span> word2vec</a><ul>
<li><a href="#cbow和skip-gram"><span class="toc-section-number">2.1</span> CBOW和skip-gram</a></li>
<li><a href="#sigmod函数"><span class="toc-section-number">2.2</span> sigmod函数</a></li>
<li><a href="#参考-1"><span class="toc-section-number">2.3</span> 参考</a></li>
</ul></li>
<li><a href="#fasttext"><span class="toc-section-number">3</span> fastText</a></li>
<li><a href="#doc2vec"><span class="toc-section-number">4</span> doc2vec</a></li>
<li><a href="#参考-2"><span class="toc-section-number">5</span> 参考</a></li>
</ul>
</nav>
<p>自然语言处理中，单词并不能直接输入模型进行处理，因此需要将单词转换为词向量输入模型进行计算，但是词向量的转换方式需要不仅考虑词的代表性（比如 one-hot 编码），还需要考虑词的语义信息（相近词的词向量距离较近，比如 word2vec ），因此 <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> 即是将词转换为包含尽可能多的语义信息的向量的过程。</p>
<p>将单词表示为词向量后，有多种方式可以用来通过词向量来表示整个句子甚至整篇文章：</p>
<ul>
<li>Word Centroid Vector: 将所有词向量加权平均，最终得到的向量就是整个文本在词向量空间中的质心；</li>
<li>Continuous Bag of Words (CBOW): 将定长窗口内的词向量首尾连接，便构成了这一窗口内文本的向量表示，常用方案之一；</li>
<li>Word Mover’s Distance (WMD): 将词间的相似度使用 word2vec 向量间的欧式距离衡量，句子间的相似度通过求解经典的 transportation problem 优化问题得到。缺点是：对句子的长度敏感，长度相差很大的句子之间进行计算时开销较大；其次整体的计算开销也较大。</li>
</ul>
<h1 id="glove-global-vector"><span class="header-section-number">1</span> GloVe (GLObal VEctor)</h1>
<h2 id="参考"><span class="header-section-number">1.1</span> 参考</h2>
<ul>
<li><a href="https://nlp.stanford.edu/projects/glove/">GloVe: Global Vectors for Word Representation</a></li>
</ul>
<h1 id="word2vec"><span class="header-section-number">2</span> word2vec</h1>
<p>word2vec 采用的是 n-gram model ，即假设一个词只与周围n个词有关：</p>
<blockquote>
<p>The meaning of a word can be inferred by the company it keeps.</p>
</blockquote>
<blockquote>
<p>Show me your friends, and I’ll tell who you are.</p>
</blockquote>
<p><a href="http://kavita-ganesan.com/gensim-word2vec-tutorial-starter-code/#.W2PHZ8Jx2Uk">Gensim Word2Vec Tutorial – Full Working Example</a></p>
<h2 id="cbow和skip-gram"><span class="header-section-number">2.1</span> CBOW和skip-gram</h2>
<p>CBOW 模型根据输入周围 n-1 个词来预测出这个词本身，而 skip-gram 模型则能够根据词本身来预测周围有哪些词。</p>
<h2 id="sigmod函数"><span class="header-section-number">2.2</span> sigmod函数</h2>
<p>sigmod函数的到函数拥有以下形式：</p>
<p><span class="math display">\[\sigma&#39;(x)=\frac{1}{1+e^{-x}}&#39;=(\frac{1}{1+e^{-x}}){\cdot}(\frac{e^{-x}}{1+e^{-x}})=\sigma(x)[1-\sigma(x)]\]</span></p>
<p>并由此得出 <span class="math inline">\(\log\sigma(x)\)</span> 和 <span class="math inline">\(\log(1-\sigma(x))\)</span> 的导函数分别为：</p>
<p><span class="math display">\[(\log\sigma(x))&#39;=1-\sigma(x)\]</span></p>
<p><span class="math display">\[(\log(1-\sigma(x)))&#39;=-\sigma(x)\]</span></p>
<h2 id="参考-1"><span class="header-section-number">2.3</span> 参考</h2>
<ul>
<li><a href="http://www.cnblogs.com/peghoty/p/3857839.html">word2vec中的数学原理详解</a></li>
<li><a href="https://www.zybuluo.com/Dounm/note/591752">Word2Vec-知其然知其所以然</a></li>
</ul>
<h1 id="fasttext"><span class="header-section-number">3</span> fastText</h1>
<h1 id="doc2vec"><span class="header-section-number">4</span> doc2vec</h1>
<h1 id="参考-2"><span class="header-section-number">5</span> 参考</h1>
<ul>
<li><a href="https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html">Using pre-trained word embeddings in a Keras model</a></li>
</ul>
</body>
</html>
