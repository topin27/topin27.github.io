<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Notes about Spark</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <style>
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="/static/custom.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Notes about Spark</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#资源"><span class="toc-section-number">1</span> 资源</a><ul>
<li><a href="#spark中文文档"><span class="toc-section-number">1.1</span> <span>Spark中文文档</span></a></li>
</ul></li>
<li><a href="#概览"><span class="toc-section-number">2</span> 概览</a><ul>
<li><a href="#初始化spark"><span class="toc-section-number">2.1</span> 初始化spark</a></li>
<li><a href="#rdd"><span class="toc-section-number">2.2</span> RDD</a><ul>
<li><a href="#外部数据库"><span class="toc-section-number">2.2.1</span> 外部数据库</a></li>
<li><a href="#rdd操作"><span class="toc-section-number">2.2.2</span> RDD操作</a></li>
<li><a href="#闭包"><span class="toc-section-number">2.2.3</span> 闭包</a></li>
<li><a href="#常用的transformation"><span class="toc-section-number">2.2.4</span> 常用的transformation</a></li>
<li><a href="#常用的action"><span class="toc-section-number">2.2.5</span> 常用的action</a></li>
<li><a href="#shuffle操作"><span class="toc-section-number">2.2.6</span> Shuffle操作</a></li>
<li><a href="#rdd持久化"><span class="toc-section-number">2.2.7</span> RDD持久化</a></li>
</ul></li>
<li><a href="#共享变量"><span class="toc-section-number">2.3</span> 共享变量</a><ul>
<li><a href="#广播变量"><span class="toc-section-number">2.3.1</span> 广播变量</a></li>
<li><a href="#累加器"><span class="toc-section-number">2.3.2</span> 累加器</a></li>
</ul></li>
<li><a href="#参考"><span class="toc-section-number">2.4</span> 参考</a></li>
</ul></li>
<li><a href="#time-window"><span class="toc-section-number">3</span> time window</a></li>
<li><a href="#pysaprk"><span class="toc-section-number">4</span> pysaprk</a><ul>
<li><a href="#pyspark中调用第三方jar"><span class="toc-section-number">4.1</span> pyspark中调用第三方jar</a></li>
<li><a href="#使用list类型为dataframe添加一列"><span class="toc-section-number">4.2</span> 使用list类型为Dataframe添加一列</a></li>
<li><a href="#简单的判断条件可以不使用udf生成新列"><span class="toc-section-number">4.3</span> 简单的判断条件可以不使用udf生成新列</a></li>
</ul></li>
<li><a href="#dataframe"><span class="toc-section-number">5</span> DataFrame</a></li>
</ul>
</nav>
<h1 id="资源"><span class="header-section-number">1</span> 资源</h1>
<h2 id="spark中文文档"><span class="header-section-number">1.1</span> <a href="http://spark.apachecn.org/docs/cn/2.2.0/">Spark中文文档</a></h2>
<p>Spark官方文档的中文翻译</p>
<h1 id="概览"><span class="header-section-number">2</span> 概览</h1>
<h2 id="初始化spark"><span class="header-section-number">2.1</span> 初始化spark</h2>
<p>初始化时设置master，其值可以为Spark、Mesos、YARN或者local，其中local表示在本地模式下运行。</p>
<h2 id="rdd"><span class="header-section-number">2.2</span> RDD</h2>
<h3 id="外部数据库"><span class="header-section-number">2.2.1</span> 外部数据库</h3>
<p>spark中如果读取本地的数据，必须保证其他worker节点上同路径下也有该文件，并且有访问权限。</p>
<h3 id="rdd操作"><span class="header-section-number">2.2.2</span> RDD操作</h3>
<p>两种类型的操作</p>
<ul>
<li>transformation：从一个RDD转换为一个新的RDD，比如map</li>
<li>action：基于一个数据集进行运算，返回RDD，比如reduce</li>
</ul>
<p>所有的transformation都是lazy的，在执行一个需要执行的action时，才会执行数据集上所有的transformation。</p>
<p>默认情况下，每个转换的RDD在执行action操作时都会重新计算，即使两个action操作会使用同样的RDD，这种情况下，可以使用 <code>persist</code> 或 <code>cache</code> 方法缓存至内存。</p>
<h3 id="闭包"><span class="header-section-number">2.2.3</span> 闭包</h3>
<p>本地模式下仅有一个JVM，因此可以直接使用一些全局的变量，但是在集群模式下，情况变得复杂，这种情况下则尽量使用闭包。集群模式下，闭包会被序列化，并发送给每个executor。</p>
<p>在分布式下运行时，建议使用累加器定义一些全局集合。</p>
<h3 id="常用的transformation"><span class="header-section-number">2.2.4</span> 常用的transformation</h3>
<ul>
<li>map(func)</li>
<li>filter(func)</li>
<li>flatMap(func)</li>
<li>mapPartitions(func)</li>
<li>mapPartitionsWithIndex(func)</li>
<li>sample(withReplacement, fraction, seed)</li>
<li>union(otherDataset)：返回原数据集和指定数据集合并后的数据集</li>
<li>intersection(otherDataset)</li>
<li>distinct([numTasks])</li>
<li>groupByKey([numTasks])：操作(K,V)格式的数据集，返回(K,Iterable)格式的数据集</li>
<li>reduceByKey(func, [numTasks])</li>
<li>aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])：对(K,V)格式的数据按key进行聚合，聚合时使用给定的合并函数和一个初始值</li>
<li>sortByKey([ascending], [numTasks])</li>
<li>join(otherDataset, [numTasks])：操作两个数据集(K,V)和(K,W)返回(K,(V,W))</li>
<li>cogroup(otherDataset, [numTasks])</li>
<li>cartesian(otherDataset)：操作数据集T和U，返回包含两个数据集所有元素的(T,U)数据集，即对两个RDD做笛卡尔积操作</li>
<li>pipe(command, [envVars])：以管道方式将RDD各个分区使用shell命令处理</li>
<li>coalesce(numPartitions)</li>
<li>repartition(numPartitions)</li>
<li>repartitionAndSortWithinPartitions(partitioner)</li>
</ul>
<h3 id="常用的action"><span class="header-section-number">2.2.5</span> 常用的action</h3>
<ul>
<li>reduce(func)</li>
<li>collect()</li>
<li>count()</li>
<li>first()</li>
<li>take()</li>
<li>takeSample(withReplacement, num, [seed])</li>
<li>takeOrdered(n, [ordering])：返回排序后的前n个元素</li>
<li>saveAsTextFile(path)</li>
<li>saveAsSequenceFile(path)</li>
<li>saveAsObjectFile(path)：将数据集中的元素以简单的java序列化到指定的路径，后续可以使用 <code>SparkContext.objectFile()</code> 进行加载</li>
<li>countByKey()</li>
<li>foreach(func)</li>
</ul>
<h3 id="shuffle操作"><span class="header-section-number">2.2.6</span> Shuffle操作</h3>
<p>Shuffle是spark将多个分区的数据重新分组重新分布数据的机制，该操作复杂且代价高，因为需要完成数据在executor和机器节点之间的复制工作。</p>
<p>引起shuffle的操作有：</p>
<ul>
<li><code>repartition</code> 操作，例如 <code>repartition</code> 、 <code>coalesce</code></li>
<li><code>ByKey</code> 操作</li>
<li><code>join</code> 操作，例如 <code>cogroup</code> 、 <code>join</code></li>
</ul>
<h3 id="rdd持久化"><span class="header-section-number">2.2.7</span> RDD持久化</h3>
<p>持久化后，每个节点会将本节点计算的数据块存储到内存，在该数据上的其他action将直接使用内存中的数据。缓存是迭代算法和快速的交互使用的重要工具。</p>
<p>spark的缓存具有容错机制，如果一个缓冲的RDD的某个分区丢失了，Spark将按照原来的计算过程，自动重新计算并进行缓存。</p>
<h2 id="共享变量"><span class="header-section-number">2.3</span> 共享变量</h2>
<p>通常情况下，传递给spark操作的方法是在远程集群上的节点执行的，节点执行过程中使用的变量，是同一份变量的多个副本，各个副本的更新并不会传回driver程序。</p>
<p>有两种特定类型的共享变量：广播变量、累加器。</p>
<h3 id="广播变量"><span class="header-section-number">2.3.1</span> 广播变量</h3>
<p>广播变量将一个只读变量缓存到每个机器上，而不是给每个任务一个副本。</p>
<h3 id="累加器"><span class="header-section-number">2.3.2</span> 累加器</h3>
<p>累加器可以用于实现计数或者求和，只有driver程序可以读取累加器的值。</p>
<p>累加器的更新至发生在action操作中，Spark保证每个任务只能更新累加器一次。</p>
<h2 id="参考"><span class="header-section-number">2.4</span> 参考</h2>
<ul>
<li><a href="http://www.cnblogs.com/BYRans/p/5292763.html">Spark官方文档 - 中文翻译</a></li>
</ul>
<h1 id="time-window"><span class="header-section-number">3</span> time window</h1>
<p>所有time window相关的api都需要一个timestamp的列。</p>
<h1 id="pysaprk"><span class="header-section-number">4</span> pysaprk</h1>
<h2 id="pyspark中调用第三方jar"><span class="header-section-number">4.1</span> pyspark中调用第三方jar</h2>
<p><a href="http://aseigneurin.github.io/2016/09/01/spark-calling-scala-code-from-pyspark.html">Spark - Calling Scala code from PySpark</a></p>
<p>(INVALID)</p>
<pre class="example"><code>    pyspark --jars file1.jar,file2.jar
     # and
    ./bin/spark-submit --jars xxx.jar your_spark_script.py
     # or
    SPARK_CLASSPATH=&#39;/path/xxx.jar:/path/xx2.jar&#39; your_spark_script.py</code></pre>
<p>(VALID) 或者另外一种方式：</p>
<pre class="example"><code>    pyspark --driver-class-path .../target/spark-kafka-source-0.2.0-SNAPSHOT.jar</code></pre>
<p>调用时使用：</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb3-1" title="1">    sc._jvm.com.ippontech.Hello.hello()</a></code></pre></div>
<p>这种方式调用时，向其中的函数参数传入DataFrame时需要转换为jdf（ <code>df._jdf</code> ）。</p>
<h2 id="使用list类型为dataframe添加一列"><span class="header-section-number">4.2</span> 使用list类型为Dataframe添加一列</h2>
<p>使用list创建一个dataframe，然后为每个dataframe创建一个“rownum”，以此进行合并：</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb4-1" title="1">    <span class="im">from</span> pyspark.sql <span class="im">import</span> Row</a>
<a class="sourceLine" id="cb4-2" title="2"></a>
<a class="sourceLine" id="cb4-3" title="3">    <span class="kw">def</span> flatten_row(r):</a>
<a class="sourceLine" id="cb4-4" title="4">        r_ <span class="op">=</span>  r.features.asDict()</a>
<a class="sourceLine" id="cb4-5" title="5">        r_.update({<span class="st">&#39;row_num&#39;</span>: r.row_num})</a>
<a class="sourceLine" id="cb4-6" title="6">        <span class="cf">return</span> Row(<span class="op">**</span>r_)</a>
<a class="sourceLine" id="cb4-7" title="7"></a>
<a class="sourceLine" id="cb4-8" title="8">    <span class="kw">def</span> add_row_num(df):</a>
<a class="sourceLine" id="cb4-9" title="9">        df_row_num <span class="op">=</span> df.rdd.zipWithIndex().toDF([<span class="st">&#39;features&#39;</span>, <span class="st">&#39;row_num&#39;</span>])</a>
<a class="sourceLine" id="cb4-10" title="10">        df_out <span class="op">=</span> df_row_num.rdd.<span class="bu">map</span>(<span class="kw">lambda</span> x : flatten_row(x)).toDF()</a>
<a class="sourceLine" id="cb4-11" title="11">        <span class="cf">return</span> df_out</a>
<a class="sourceLine" id="cb4-12" title="12"></a>
<a class="sourceLine" id="cb4-13" title="13">    df_x4 <span class="op">=</span> spark.createDataFrame([Row(<span class="op">**</span>{<span class="st">&#39;x4&#39;</span>: x}) <span class="cf">for</span> x <span class="kw">in</span> [i <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">7</span>)]])</a>
<a class="sourceLine" id="cb4-14" title="14"></a>
<a class="sourceLine" id="cb4-15" title="15">    df <span class="op">=</span> add_row_num(df)</a>
<a class="sourceLine" id="cb4-16" title="16">    df_x4 <span class="op">=</span> add_row_num(df_x4)</a>
<a class="sourceLine" id="cb4-17" title="17">    df_concat <span class="op">=</span> df.join(df_x4, on<span class="op">=</span><span class="st">&#39;row_num&#39;</span>).drop(<span class="st">&#39;row_num&#39;</span>)</a></code></pre></div>
<h2 id="简单的判断条件可以不使用udf生成新列"><span class="header-section-number">4.3</span> 简单的判断条件可以不使用udf生成新列</h2>
<div class="sourceCode" id="cb5"><pre class="sourceCode python"><code class="sourceCode python"><a class="sourceLine" id="cb5-1" title="1">    pattern_df <span class="op">=</span> pattern_df.select(<span class="st">&#39;*&#39;</span>, F.when(F.col(<span class="st">&#39;NeedCorrect&#39;</span>).isNull(), F.col(<span class="st">&#39;sup&#39;</span>)).otherwise(F.col(<span class="st">&#39;sup&#39;</span>) <span class="op">*</span> weight).alias(<span class="st">&#39;CorrectSup&#39;</span>))</a></code></pre></div>
<h1 id="dataframe"><span class="header-section-number">5</span> DataFrame</h1>
</body>
</html>
