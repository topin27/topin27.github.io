<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Online Learning Papers</title>
  <style>
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      span.underline{text-decoration: underline;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="/static/custom.css" />
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header id="title-block-header">
<h1 class="title">Online Learning Papers</h1>
</header>
<nav id="TOC" role="doc-toc">
<ul>
<li><a href="#文献"><span class="toc-section-number">1</span> 文献</a><ul>
<li><a href="#done-beygelzimer-a-kale-s-luo-h.-optimal-and-adaptive-algorithms-for-online-boostingj.-computer-science-20152323-2331."><span class="toc-section-number">1.1</span> DONE <span>Beygelzimer A, Kale S, Luo H. Optimal and Adaptive Algorithms for Online Boosting{J}. Computer Science, 2015:2323-2331.</span></a><ul>
<li><a href="#introduction"><span class="toc-section-number">1.1.1</span> Introduction</a></li>
<li><a href="#setup-and-assumptions"><span class="toc-section-number">1.1.2</span> Setup and Assumptions</a></li>
<li><a href="#an-optimal-algorithm"><span class="toc-section-number">1.1.3</span> An Optimal Algorithm</a></li>
</ul></li>
<li><a href="#todo-tsymbal-a.-the-problem-of-concept-drift-definitions-and-related-workj.-technical-report-department-of-computer-science-trinity-college-dublin-2004."><span class="toc-section-number">1.2</span> TODO <span>Tsymbal A. The problem of concept drift : definitions and related work{J}. Technical Report Department of Computer Science Trinity College Dublin, 2004.</span></a></li>
<li><a href="#todo-žliobait.-e-e-i.-learning-under-concept-drift-an-overviewj.-computer-science-2010."><span class="toc-section-number">1.3</span> TODO <span>Žliobait. e, E I. Learning under Concept Drift: an Overview{J}. Computer Science, 2010.</span></a></li>
<li><a href="#todo-minku-l-l.-online-ensemble-learning-in-the-presence-of-concept-driftj.-university-of-birmingham-2011."><span class="toc-section-number">1.4</span> TODO <span>Minku L L. ONLINE ENSEMBLE LEARNING IN THE PRESENCE OF CONCEPT DRIFT{J}. University of Birmingham, 2011.</span></a></li>
<li><a href="#done-萧嵘-王继成-孙正兴等.-一种svm增量学习算法α-isvmj.-软件学报-2001-12121818-1824."><span class="toc-section-number">1.5</span> DONE <span>萧嵘, 王继成, 孙正兴,等. 一种SVM增量学习算法α-ISVM{J}. 软件学报, 2001, 12(12):1818-1824.</span></a></li>
<li><a href="#todo-侯杰-茅耀斌-孙金生.-基于指数损失和0-1损失的在线boosting算法j.-自动化学报-20144635-642."><span class="toc-section-number">1.6</span> TODO <span>侯杰, 茅耀斌, 孙金生. 基于指数损失和0-1损失的在线Boosting算法{J}. 自动化学报, 2014(4):635-642.</span></a></li>
<li><a href="#done-张文生-于廷照.-boosting算法理论与应用研究j."><span class="toc-section-number">1.7</span> DONE 张文生, 于廷照. Boosting算法理论与应用研究[J].</a></li>
<li><a href="#todo-pi-t-li-x-zhang-z-et-al.-self-paced-boost-learning-for-classificationc-international-joint-conference-on-artificial-intelligence.-aaai-press-20161932-1938."><span class="toc-section-number">1.8</span> TODO <span>Pi T, Li X, Zhang Z, et al. Self-paced boost learning for classification{C}// International Joint Conference on Artificial Intelligence. AAAI Press, 2016:1932-1938.</span></a></li>
<li><a href="#todo-luo-h.-optimal-and-adaptive-online-learning"><span class="toc-section-number">1.9</span> TODO <span>Luo H. Optimal and Adaptive Online Learning</span></a></li>
</ul></li>
</ul>
</nav>
<h1 id="文献"><span class="header-section-number">1</span> 文献</h1>
<h2 id="done-beygelzimer-a-kale-s-luo-h.-optimal-and-adaptive-algorithms-for-online-boostingj.-computer-science-20152323-2331."><span class="header-section-number">1.1</span> DONE <a href="https://arxiv.org/pdf/1502.02651.pdf">Beygelzimer A, Kale S, Luo H. Optimal and Adaptive Algorithms for Online Boosting{J}. Computer Science, 2015:2323-2331.</a></h2>
<p>vowpal wabbit中关于boosting部分实现的理论依据。其对应的<a href="http://www-bcf.usc.edu/~haipengl/papers/OB_slides.pdf">pdf</a> ，最终发布的<a href="http://www.ijcai.org/Proceedings/16/Papers/614.pdf">版本</a> 里，相对简化了很多证明。</p>
<h3 id="introduction"><span class="header-section-number">1.1.1</span> Introduction</h3>
<p>In contrast to the batch setting, online learning algorithms typically don’t make any stochastic assumptions about the data they observe.</p>
<p>A simple interpretation of this drawback is that all these algorithms require using (<em>γ</em>) , an unknown quantity, as a parameter. More importantly, this also means that the algorithm treats each weak learner equally and ignores the fact that some weak learners are actually doing better than the others.</p>
<h3 id="setup-and-assumptions"><span class="header-section-number">1.1.2</span> Setup and Assumptions</h3>
<p>For parameters (<em>γ</em>∈(0,)), (<em>δ</em>∈(0,1)), and a constant (S&gt;0), the learner is said to be a <strong>weak online learner</strong> with edge (<em>γ</em>) and <strong>excess loss (S)</strong> if, for any <em>T</em> and for any input sequence of examples ((x_t, y_t)) for (t=1,2,…T) chosen adaptively, it generates predictions () such that with probability at list (1-<em>δ</em>), [∑_{t=1}^<em>T</em>{1{}}{≤}{(-<em>γ</em>)T+S}] The excess loss requirement is necessary since an online learner can’t be expected to predict with any accuracy with too few examples. Essentially, the excess loss (S) yields a kind of sample complexity bound: the weak learner starts obtaining a distinct edge of (<em>Ω</em>(<em>γ</em>)) over random guessing when (T).</p>
<p>booster（强模型）由多个弱模型（(WL<sup><em>i</em>))组成，每个弱模型都有一定的权重（(<em>α</em>_t<sup>i)），每来一个新样本进行更新时，booster都会以(p_t</sup>i)的概率将新样本传递给第i个弱分类器进行更新，这里(p_t</sup><em>i</em>=)。</p>
<blockquote>
<p>定理1</p>
<p>There is a constant (=2S+()) such that for any T, with high probability, for every weak learner (WL^<em>i</em>) we have [{w<sup>i</sup>{⋅}{z}i}{≥}{<em>γ</em>}{|{w^<em>i</em>}|_1}-]</p>
</blockquote>
<ol type="1">
<li><p>Handling Importance Weights</p>
<p>典型的在线学习算法都可以为每个样本赋予一定的权重，因此前面的不等式可以改写为：the online learner generates predictions () such that with probability at least (1-<em>δ</em>), [∑/{t=1}^<em>T</em>{{p_t}1{}{≤}{(-<em>γ</em>)∑/{t=1}<sup>{T}{p_t}+S</sup>}] 因此对于这种权重问题可以简单的转换为对每个样本以概率(p_t}i=)传递给每个弱分类器。因此定理1可以简化为 [{w<sup>i</sup>{⋅}{z}i}{≥}{2<em>γ</em>}{|{w^<em>i</em>}|_1}-2]</p></li>
<li><p>Discussion of Weak Online Learning Assumption</p>
<p>In the standard batch boosting case, the corresponding weak learning assumption made is that there is an algorithm which, given a training set of examples and an arbitrary distribution on it, generates a hypothesis that has error at most (-<em>γ</em>) on the training data under the given distribution.</p>
<p>简单说来弱分类器的效果至少比随机猜测（准确度50%）好。这些假设可以归纳为：</p>
<ol type="1">
<li>(Richness.) Given an edge parameter ({<em>γ</em>}{∈}{(0,)}), there is a set of hypothesis, (ℋ), such that given any training set (possibly, a multiset) of examples (U), there is some hypothesis (h{∈}ℋ) with error at most (-<em>γ</em>), i.e. [{∑_{(x,y)∈<em>U</em>}{1{h(x){≠}y}}{≤}{-<em>γ</em>}{|U|}}.]</li>
<li>(Agnostic Learnability.) For any (<em>ϵ</em>∈(0,1)), there is an algorithm which, given any training set (possibly, a multiset) of examples <em>U</em> , can compute a nearly optimal hypothesis (h∈ℋ), i.e. [∑/{(x,y)∈<em>U</em>}{1{h(x){≠}y}}{≤}{}{∑/{(x,y){∈}{U}}{1{h’(x){≠}y}}+{<em>ϵ</em>}|U|}]</li>
</ol>
<p>本文提出的在线弱学习器基于以上的假设，但是Agnostic Learnability有所改进：There is an online learning algorithm which, given any sequence of examples, (x_t,y_t) for (t=1,2,…,T),generates predictions () such that [∑/{t=1}^<em>T</em>{1{}} ≠ ∑/{t=1}^<em>T</em> 1{h(x_t) ≠<em>y</em><sub><em>t</em></sub>}+R(T)] where (R:ℕ →ℝ_+) is the regret, a non-decreasing, subliner function of the number of prediction periods <em>T</em> .</p></li>
</ol>
<h3 id="an-optimal-algorithm"><span class="header-section-number">1.1.3</span> An Optimal Algorithm</h3>
<ol type="1">
<li>A Potential Based Family and Boost-By-Majority</li>
</ol>
<h2 id="todo-tsymbal-a.-the-problem-of-concept-drift-definitions-and-related-workj.-technical-report-department-of-computer-science-trinity-college-dublin-2004."><span class="header-section-number">1.2</span> TODO <a href="https://www.scss.tcd.ie/publications/tech-reports/reports.04/TCD-CS-2004-15.pdf">Tsymbal A. The problem of concept drift : definitions and related work{J}. Technical Report Department of Computer Science Trinity College Dublin, 2004.</a></h2>
<p>概念偏移的定义以及相关的问题难点，论文有点老（04年的）</p>
<h2 id="todo-žliobait.-e-e-i.-learning-under-concept-drift-an-overviewj.-computer-science-2010."><span class="header-section-number">1.3</span> TODO <a href="http://svn.ucc.asn.au:8080/oxinabox/Uni%2520Notes/honours/Background%2520Reading/zliobaite2009learning.pdf">Žliobait. e, E I. Learning under Concept Drift: an Overview{J}. Computer Science, 2010.</a></h2>
<p>也是一篇偏概述性的论文。</p>
<h2 id="todo-minku-l-l.-online-ensemble-learning-in-the-presence-of-concept-driftj.-university-of-birmingham-2011."><span class="header-section-number">1.4</span> TODO <a href="http://etheses.bham.ac.uk/1334/1/Minku11PhD.pdf">Minku L L. ONLINE ENSEMBLE LEARNING IN THE PRESENCE OF CONCEPT DRIFT{J}. University of Birmingham, 2011.</a></h2>
<p>伯明翰大学的毕业论文，比较长，可以就目录按图索骥挑着看。</p>
<h2 id="done-萧嵘-王继成-孙正兴等.-一种svm增量学习算法α-isvmj.-软件学报-2001-12121818-1824."><span class="header-section-number">1.5</span> DONE <a href="http://www.jos.org.cn//ch/reader/create_pdf.aspx?file_no%3D20011211&amp;journal_id%3Djos">萧嵘, 王继成, 孙正兴,等. 一种SVM增量学习算法α-ISVM{J}. 软件学报, 2001, 12(12):1818-1824.</a></h2>
<p>论文中将SV集定义为样本中支持向量的那一部分数据集，进一步将SV集分为了两类，一类是BSV（boundary support vector），代表了所有不能被正确分类的样本量，另一类是代表的是能正确分类的样本，基于此提出了两种增量的SVM算法：</p>
<ol type="1">
<li>第一种是SISVM（Simple incremental SVM）</li>
<li>第二种是(<em>α</em>)-ISVM</li>
</ol>
<p>其中第一种的流程为：</p>
<ol type="1">
<li>使用分类器(<em>Γ</em>^1)对增量样本集B进行分类，可将B划分为测试错误集(B_err)和测试正确集(B_ok)；</li>
<li>将集合(A_{sv}<sup>{1})和(B_err)的并集(A</sup>1)作为新的训练集，得到新的分类器(<em>Γ</em><sup>2)和SV集(A_{sv}</sup>2)，并将集合A中除去SV集的剩余样本与集合(B_{ok})合并在一起，微信生成的分类器(<em>Γ</em>^2)构建新的增量样本集B；</li>
<li>继续多次迭代</li>
</ol>
<p>第二种则是将样本分为了内样本（从未入选过任何的SV集），边界样本（每一次都是在SV集内），和准边界样本（偶尔出现在SV集中），通过引入遗忘因子(<em>α</em>)，在后续的迭代中去除内样本的计算，从而减少计算量。</p>
<h2 id="todo-侯杰-茅耀斌-孙金生.-基于指数损失和0-1损失的在线boosting算法j.-自动化学报-20144635-642."><span class="header-section-number">1.6</span> TODO <a href="http://www.oalib.com/paper/4417985">侯杰, 茅耀斌, 孙金生. 基于指数损失和0-1损失的在线Boosting算法{J}. 自动化学报, 2014(4):635-642.</a></h2>
<h2 id="done-张文生-于廷照.-boosting算法理论与应用研究j."><span class="header-section-number">1.7</span> DONE 张文生, 于廷照. Boosting算法理论与应用研究[J].</h2>
<p>中国科学技术大学学报, 2016(3):222-230.</p>
<p>各种主流boosting的算法流程介绍。</p>
<h2 id="todo-pi-t-li-x-zhang-z-et-al.-self-paced-boost-learning-for-classificationc-international-joint-conference-on-artificial-intelligence.-aaai-press-20161932-1938."><span class="header-section-number">1.8</span> TODO <a href="http://www.isee.zju.edu.cn/dsec/pdf/ijcai16_586.pdf">Pi T, Li X, Zhang Z, et al. Self-paced boost learning for classification{C}// International Joint Conference on Artificial Intelligence. AAAI Press, 2016:1932-1938.</a></h2>
<p>具有 <strong>/自学习/</strong> 的集成学习分类算法，<a href="https://zhuanlan.zhihu.com/p/28904764">这里</a> 还有一篇对其进行简单解读的blog。</p>
<h2 id="todo-luo-h.-optimal-and-adaptive-online-learning"><span class="header-section-number">1.9</span> TODO <a href="http://www-bcf.usc.edu/~haipengl/papers/thesis.pdf">Luo H. Optimal and Adaptive Online Learning</a></h2>
<p>online boosting的作者之一的学位论文。</p>
</body>
</html>
